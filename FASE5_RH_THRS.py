# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VnSg_0w9u6NizkrTiu4m4ZIoZ7K5zuXX
"""



"""
## **Criação e importe** ## . Criação da base unificada
Construindo um DataFrame com:
vaga_id | candidato_id | requisitos_da_vaga | perfil_do_candidato | contratado (label)"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

"""# 1. Carregar os dados


"""

vagas = pd.read_json("vagas.json")
prospects = pd.read_json("prospects.json")
applicants = pd.read_json("applicants.json")
#vagas

"""# 2. Montar dataset de treinamento
# (essa parte precisa fazer o merge baseado nos códigos de vaga e candidatos)
# Exemplo: jobs['5185']['perfil_vaga']['nivel_ingles'], applicants['31000']['formacao_e_idiomas']['nivel_ingles']


# 3. Criar features (exemplo com TF-IDF para CV)
"""

tfidf = TfidfVectorizer(max_features=100)
cv_texts = [applicants[c]['cv_pt'] for c in applicants]
cv_matrix = tfidf.fit_transform(cv_texts)

"""# 4. Treinando modelo simples

"""

X = cv_matrix

# Create a list of applicant codes from the applicants DataFrame columns
applicant_codes = applicants.columns.tolist()

# Create a set of all prospect codes from the prospects DataFrame
prospect_codes = set()
for col in prospects.columns:
    for prospect in prospects[col]['prospects']:
        prospect_codes.add(prospect['codigo'])

# Create the target variable y based on whether each applicant is in the prospect_codes set
y = [1 if code in prospect_codes else 0 for code in applicant_codes]

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)

model = RandomForestClassifier()
model.fit(X_train, y_train)

"""##Leitura e estruturação básica dos arquivos JSON Assumindo que os arquivos vagas.json, Applicants.jsonestão Prospects.jsonno mesmo diretório do script:


"""

import json
import pandas as pd

"""
# Leitura dos arquivos"""

with open('vagas.json', 'r', encoding='utf-8') as f:
    jobs_data = json.load(f)

with open('applicants.json', 'r', encoding='utf-8') as f:
    applicants_data = json.load(f)

with open('prospects.json', 'r', encoding='utf-8') as f:
    prospects_data = json.load(f)

"""
##2. Gerando o conjunto de dados final Informações principais da vaga (exigências) Perfil do candidato (formação, idiomas, conhecimentos técnicos, CV)Rótulo: 1se situacao_candidatofor algo como "Contratado"ou similar."""

import json
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import nltk
from nltk.corpus import stopwords

# Load data from JSON files
with open('vagas.json', 'r', encoding='utf-8') as f:
    jobs_data = json.load(f)

with open('applicants.json', 'r', encoding='utf-8') as f:
    applicants_data = json.load(f)

with open('prospects.json', 'r', encoding='utf-8') as f:
    prospects_data = json.load(f)

data = []

print(f"Number of entries in prospects_data: {len(prospects_data)}")

for vaga_id, vaga_info in prospects_data.items():
    # Ignora se não estiver em Jobs.json - REMOVED
    # if vaga_id not in jobs_data:
    #     continue

    vaga = jobs_data.get(vaga_id, {}) # Use .get to avoid KeyError if vaga_id is not in jobs_data
    perfil_vaga = vaga.get('perfil_vaga', {})

    print(f"Processing vaga_id: {vaga_id}")

    prospects_list = vaga_info.get('prospects', [])
    print(f"Number of prospects for vaga_id {vaga_id}: {len(prospects_list)}")

    for prospect in prospects_list:
        candidato_id = prospect['codigo']
        situacao = prospect['situacao_candidado'].lower()

        # Ignora se não estiver em applicants.json - REMOVED
        # if candidato_id not in applicants_data:
        #     continue

        candidato = applicants_data.get(candidato_id, {})

        # Consider 'contratado' and 'contratado pela decision' as hired
        contratado = situacao in ['contratado', 'contratado pela decision']
        dados = {
            'vaga_id': vaga_id,
            'candidato_id': candidato_id,
            'situacao': situacao,
            'contratado': contratado,

            # Requisitos da vaga
            'vaga_nivel_profissional': perfil_vaga.get('nivel profissional', ''),
            'vaga_nivel_ingles': perfil_vaga.get('nivel_ingles', ''),
            'vaga_nivel_espanhol': perfil_vaga.get('nivel_espanhol', ''),
            'vaga_nivel_academico': perfil_vaga.get('nivel_academico', ''),
            'vaga_areas_atuacao': perfil_vaga.get('areas_atuacao', ''),
            'vaga_competencias': perfil_vaga.get('competencia_tecnicas_e_comportamentais', ''),

            # Informações do candidato
            'candidato_nivel_profissional': candidato.get('informacoes_profissionais', {}).get('nivel_profissional', ''),
            'candidato_nivel_ingles': candidato.get('formacao_e_idiomas', {}).get('nivel_ingles', ''),
            'candidato_nivel_espanhol': candidato.get('formacao_e_idiomas', {}).get('nivel_espanhol', ''),
            'candidato_nivel_academico': candidato.get('formacao_e_idiomas', {}).get('nivel_academico', ''),
            'candidato_area_atuacao': candidato.get('informacoes_profissionais', {}).get('area_atuacao', ''),
            'candidato_conhecimentos': candidato.get('informacoes_profissionais', {}).get('conhecimentos_tecnicos', ''),
            'candidato_cv': candidato.get('cv_pt', '')
        }

        data.append(dados)

print(f"\nFinal number of entries in data list: {len(data)}")

# Define the expected columns for the DataFrame
expected_columns = [
    'vaga_id', 'candidato_id', 'situacao', 'contratado',
    'vaga_nivel_profissional', 'vaga_nivel_ingles', 'vaga_nivel_espanhol',
    'vaga_nivel_academico', 'vaga_areas_atuacao', 'vaga_competencias',
    'candidato_nivel_profissional', 'candidato_nivel_ingles', 'candidato_nivel_espanhol',
    'candidato_nivel_academico', 'candidato_area_atuacao', 'candidato_conhecimentos',
    'candidato_cv'
]

# Create DataFrame from data with explicitly defined columns
df = pd.DataFrame(data, columns=expected_columns)

# 1. Preenchendo valores nulos e padronizando texto (for structured columns)
for col in [
    'vaga_nivel_profissional', 'vaga_nivel_ingles', 'vaga_nivel_espanhol',
    'vaga_nivel_academico', 'vaga_areas_atuacao',
    'candidato_nivel_profissional', 'candidato_nivel_ingles', 'candidato_nivel_espanhol',
    'candidato_nivel_academico', 'candidato_area_atuacao'
]:
    df[col] = df[col].fillna('').str.lower().str.strip()

# 2. Label Encoding em variáveis com poucos valores únicos
label_cols = [
    'vaga_nivel_profissional', 'vaga_nivel_ingles', 'vaga_nivel_espanhol',
    'vaga_nivel_academico', 'vaga_areas_atuacao',
    'candidato_nivel_profissional', 'candidato_nivel_ingles', 'candidato_nivel_espanhol',
    'candidato_nivel_academico', 'candidato_area_atuacao'
]

label_encoders = {}
for col in label_cols:
    le = LabelEncoder()
    df[col + '_enc'] = le.fit_transform(df[col])
    label_encoders[col] = le

# Using only structured features
X_final = df[[c + '_enc' for c in label_cols]]
y_final = df['contratado']

# 1. Dividir em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.3, random_state=42, stratify=y_final)

# 2. Treinar o modelo
# Added class_weight='balanced' to handle class imbalance
model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced')
model.fit(X_train, y_train)

# 3. Avaliação
y_pred = model.predict(X_test)

print("Relatório de Classificação:")
print(classification_report(y_test, y_pred))

# 4. Matriz de Confusão
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Não Contratado', 'Contratado'], yticklabels=['Não Contratado', 'Contratado'])
plt.xlabel('Predito')
plt.ylabel('Real')
plt.title('Matriz de Confusão')
plt.show()

contratado = situacao == 'contratado'
        dados = {
            'vaga_id': vaga_id,
            'candidato_id': candidato_id,
            'situacao': situacao,
            'contratado': contratado,

            # Requisitos da vaga
            'vaga_nivel_profissional': perfil_vaga.get('nivel profissional', ''),
            'vaga_nivel_ingles': perfil_vaga.get('nivel_ingles', ''),
            'vaga_nivel_espanhol': perfil_vaga.get('nivel_espanhol', ''),
            'vaga_nivel_academico': perfil_vaga.get('nivel_academico', ''),
            'vaga_areas_atuacao': perfil_vaga.get('areas_atuacao', ''),
            'vaga_competencias': perfil_vaga.get('competencia_tecnicas_e_comportamentais', ''),

            # Informações do candidato
            'candidato_nivel_profissional': candidato.get('informacoes_profissionais', {}).get('nivel_profissional', ''),
            'candidato_nivel_ingles': candidato.get('formacao_e_idiomas', {}).get('nivel_ingles', ''),
            'candidato_nivel_espanhol': candidato.get('formacao_e_idiomas', {}).get('nivel_espanhol', ''),
            'candidato_nivel_academico': candidato.get('formacao_e_idiomas', {}).get('nivel_academico', ''),
            'candidato_area_atuacao': candidato.get('informacoes_profissionais', {}).get('area_atuacao', ''),
            'candidato_conhecimentos': candidato.get('informacoes_profissionais', {}).get('conhecimentos_tecnicos', ''),
            'candidato_cv': candidato.get('cv_pt', '')
        }

        data.append(dados)

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# 1. Preenchendo valores nulos e padronizando texto
for col in [
    'vaga_nivel_profissional', 'vaga_nivel_ingles', 'vaga_nivel_espanhol',
    'vaga_nivel_academico', 'vaga_areas_atuacao',
    'candidato_nivel_profissional', 'candidato_nivel_ingles', 'candidato_nivel_espanhol',
    'candidato_nivel_academico', 'candidato_area_atuacao'
]:
    df[col] = df[col].fillna('').str.lower().str.strip()

# 2. Label Encoding em variáveis com poucos valores únicos
label_cols = [
    'vaga_nivel_profissional', 'vaga_nivel_ingles', 'vaga_nivel_espanhol',
    'vaga_nivel_academico', 'vaga_areas_atuacao',
    'candidato_nivel_profissional', 'candidato_nivel_ingles', 'candidato_nivel_espanhol',
    'candidato_nivel_academico', 'candidato_area_atuacao'
]

label_encoders = {}
for col in label_cols:
    le = LabelEncoder()
    df[col + '_enc'] = le.fit_transform(df[col])
    label_encoders[col] = le

# Excluindo colunas de texto cru (vamos usar no Passo 2)
X_structured = df[[c + '_enc' for c in label_cols]]
y = df['contratado']

print("Amostra dos dados estruturados e codificados:")
display(X_structured.head())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from scipy.sparse import hstack
import nltk
from nltk.corpus import stopwords


try:
    stopwords = stopwords.words('portuguese')
except LookupError:
    nltk.download('stopwords')
    stopwords = stopwords.words('portuguese')


# 1. Preencher valores nulos e padronizar
df['vaga_competencias'] = df['vaga_competencias'].fillna('').str.lower()
df['candidato_conhecimentos'] = df['candidato_conhecimentos'].fillna('').str.lower()
df['candidato_cv'] = df['candidato_cv'].fillna('').str.lower()

# 2. Criar campo combinado
df['texto_comb'] = (
    df['vaga_competencias'] + ' ' +
    df['candidato_conhecimentos'] + ' ' +
    df['candidato_cv']
)

# 3. Aplicar TF-IDF
tfidf = TfidfVectorizer(max_features=500, stop_words=stopwords)
X_tfidf = tfidf.fit_transform(df['texto_comb'])

# 4. Concatenar com as features estruturadas do passo 1
X_final = hstack([X_structured, X_tfidf])
y_final = df['contratado']

#codigo


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.preprocessing import LabelEncoder
# Importações não utilizadas removidas para processamento de texto
# from sklearn.feature_extraction.text import TfidfVectorizer
# from scipy.sparse import hstack
# import nltk
# from nltk.corpus import stopwords

# Baixar stopwords em português, caso ainda não tenham sido baixadas - Removidas, pois o processamento de texto foi removido
# try:
# stopwords = stopwords.words('portuguese')
# except LookupError:
# nltk.download('stopwords')
# stopwords = stopwords.words('portuguese')

# Definir as colunas esperadas para o DataFrame
expected_columns = [
    'vaga_id', 'candidato_id', 'situacao', 'contratado',
    'vaga_nivel_profissional', 'vaga_nivel_ingles', 'vaga_nivel_espanhol',
    'vaga_nivel_academico', 'vaga_areas_atuacao', 'vaga_competencias',
    'candidato_nivel_profissional', 'candidato_nivel_ingles', 'candidato_nivel_espanhol',
    'candidato_nivel_academico', 'candidato_area_atuacao', 'candidato_conhecimentos',
    'candidato_cv'
]


# Crie um DataFrame a partir de dados com colunas definidas explicitamente
df = pd.DataFrame(data, columns=expected_columns)


# 1. Preenchendo valores nulos e padronizando texto (para colunas estruturadas)
for col in [
    'vaga_nivel_profissional', 'vaga_nivel_ingles', 'vaga_nivel_espanhol',
    'vaga_nivel_academico', 'vaga_areas_atuacao',
    'candidato_nivel_profissional', 'candidato_nivel_ingles', 'candidato_nivel_espanhol',
    'candidato_nivel_academico', 'candidato_area_atuacao'
]:
    df[col] = df[col].fillna('').str.lower().str.strip()

# 2. Label Encoding em variáveis com poucos valores únicos
label_cols = [
    'vaga_nivel_profissional', 'vaga_nivel_ingles', 'vaga_nivel_espanhol',
    'vaga_nivel_academico', 'vaga_areas_atuacao',
    'candidato_nivel_profissional', 'candidato_nivel_ingles', 'candidato_nivel_espanhol',
    'candidato_nivel_academico', 'candidato_area_atuacao'
]

label_encoders = {}
for col in label_cols:
    le = LabelEncoder()
    df[col + '_enc'] = le.fit_transform(df[col])
    label_encoders[col] = le

# Excluir colunas de texto cru e definir X_final e y_final using the full df
X_final = df[[c + '_enc' for c in label_cols]]
y_final = df['contratado']

# Etapas de processamento de texto removidas
# df['vaga_competencias'] = df['vaga_competencias'].fillna('').str.lower()
# df['candidato_conhecimentos'] = df['candidato_conhecimentos'].fillna('').str.lower()
# df['candidato_cv'] = df['candidato_cv'].fillna('').str.lower()
# df['texto_comb'] = (
# df['vaga_competencias'] + ' ' +
# df['candidato_conhecimentos'] + ' ' +
#df['candidato_cv']
#)
# tfidf = TfidfVectorizer(max_features=500, stop_words=stopwords)
#X_tfidf = tfidf.fit_transform(df['texto_comb'])
# X_final = hstack([X_structured, X_tfidf])
# y_final = df['contratado']


# 1. Dividir em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.3, random_state=42, stratify=y_final)

# 2. Treinar o modelo
model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
model.fit(X_train, y_train)

# 3. Avaliação
y_pred = model.predict(X_test)

print("Relatório de Classificação:")
print(classification_report(y_test, y_pred))

# 4. Matriz de Confusão
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Não Contratado', 'Contratado'], yticklabels=['Não Contratado', 'Contratado'])
plt.xlabel('Predito')
plt.ylabel('Real')
plt.title('Matriz de Confusão')
plt.show()

# Prever probabilidades para todos os pontos de dados
# O modelo foi treinado em X_final, que inclui todos os pontos de dados
# Precisamos garantir que os dados usados para a previsão estejam no mesmo formato que X_final
# Como X_final foi criado a partir do df completo, podemos usar X_final diretamente para a previsão
probabilities = model.predict_proba(X_final)[:, 1]
# Obter probabilidade da classe positiva (contratada)


# Adicionar probabilidades ao DataFrame
df['predicted_proba'] = probabilities


# Encontre o melhor candidato para cada vaga_id com base na probabilidade prevista
top_candidates_per_vaga = df.loc[df.groupby('vaga_id')['predicted_proba'].idxmax()]


# Selecione e renomeie colunas para a tabela final
recommendation_table = top_candidates_per_vaga[['vaga_id', 'candidato_id', 'predicted_proba']]
recommendation_table = recommendation_table.rename(columns={
    'vaga_id': 'ID da Vaga',
    'candidato_id': 'ID do Candidato Recomendado',
    'predicted_proba': 'Probabilidade de Contratação'
})


# Classifique por probabilidade em ordem decrescente para melhor legibilidade
recommendation_table = recommendation_table.sort_values(by='Probabilidade de Contratação', ascending=False)


# Exibir a tabela de recomendações
print("Candidato mais identificado para cada vaga:")
display(recommendation_table)

"""#Salvar o modelo treinado
##Subtarefa:
Salvar o modelo treinado e os label_encoders (já que são necessários para o pré-processamento de novos dados) em arquivos.

**Raciocínio**:
Salve o modelo treinado e os codificadores de rótulos em arquivos usando joblib.
"""

import joblib

joblib.dump(model, 'random_forest_model.joblib')
joblib.dump(label_encoders, 'label_encoders.joblib')

"""## Criando um script para a aplicação Streamlit

### Subtarefa:
 Este script precisará:
- Carregar o modelo salvo e os codificadores de rótulos.
- Criaando uma interface de usuário para inserir dados da vaga e do candidato.
- Pré-processar os dados de entrada usando os codificadores de rótulos carregados.
- Usar o modelo carregado para prever a probabilidade de o candidato ser contratado para a vaga em questão.
- Exibir a probabilidade prevista e a recomendação.

**Raciocínio**:
Crie o script Python para o aplicativo Streamlit, incluindo o carregamento do modelo e dos codificadores, a definição da função de previsão, a configuração da interface do usuário e a exibição dos resultados.
"""

import streamlit as st
import joblib
import pandas as pd
import numpy as np


# Carregando o modelo salvo e os codificadores de rótulo
try:
    model = joblib.load('random_forest_model.joblib')
    label_encoders = joblib.load('label_encoders.joblib')
except FileNotFoundError:
    st.error("Model or label encoders not found. Please run the training notebook first.")
    st.stop()

def preprocess_input(job_features, candidate_features, label_encoders):
    """
    Preprocesses job and candidate features using loaded label encoders.
    Ensures the input features are in the same format as the training data.
    """
    processed_features = {}

# Combine características de emprego e candidato em um único dicionário para facilitar o processamento
    all_features = {**job_features, **candidate_features}

    for col, le in label_encoders.items():
        value = all_features.get(col, '')
# Obter o valor, padrão para string vazia se não for encontrado
        try:

# Usand o codificador de rótulo carregado para transformar o valor de entrada
# Manipulando valores não vistos atribuindo um valor padrão (por exemplo, 0 ou uma categoria dedicada 'não vista')
# Aqui, usei 0, supondo que o codificador foi instalado em um conjunto abrangente
            processed_features[col + '_enc'] = le.transform([str(value).lower().strip()])[0]
        except ValueError:

# Lidando com casos em que a transformação falha (por exemplo, rótulo não visto)
            processed_features[col + '_enc'] = 0 # Atribuir um valor padrão

# Converter para DataFrame, garantindo que a ordem das colunas corresponda aos dados de treinamento
# Precisamos obter a ordem das colunas do DataFrame X_final original usado no treinamento
# Supondo que a ordem das colunas em X_final foi baseada na ordem em label_cols

    label_cols = list(label_encoders.keys())
    encoded_cols = [c + '_enc' for c in label_cols]
    processed_df = pd.DataFrame([processed_features], columns=encoded_cols)


    return processed_df

# Layout do aplicativo Streamlit
st.title("Candidate Recommendation Engine")

st.write("Enter job and candidate details to get a hiring recommendation.")


# Widgets de entrada para recursos de trabalho (com base em label_cols)
st.header("Job Details")
job_features = {}
for col in label_encoders.keys():
    if col.startswith('vaga_'):
        options = [''] + list(label_encoders[col].classes_) # Adicionar string vazia para seleção
        job_features[col] = st.selectbox(f"Job {col.replace('vaga_', '').replace('_', ' ').title()}:", options)

# Widgets de entrada para recursos candidatos (com base em label_cols)
st.header("Candidate Details")
candidate_features = {}
for col in label_encoders.keys():
    if col.startswith('candidato_'):
        options = [''] + list(label_encoders[col].classes_) # Adicionar string vazia para seleção
        candidate_features[col] = st.selectbox(f"Candidate {col.replace('candidato_', '').replace('_', ' ').title()}:", options)


if st.button("Get Recommendation"):
    # Combinandi recursos de trabalho e candidato para pré-processamento
    input_features = {**job_features, **candidate_features}

   # Pré-processando os dados de entrada
    processed_input = preprocess_input(job_features, candidate_features, label_encoders)

 # Fazendo previsão
# Garantindo que o número de recursos corresponda à expectativa do modelo
# Se o modelo foi treinado apenas em recursos estruturados, use processed_input diretamente
# Se o modelo foi treinado em recursos estruturados + texto, precisaría processar texto aqui também
# Com base no notebook anterior, o modelo foi treinado apenas em recursos estruturados.
    if processed_input.shape[1] == model.n_features_in_:
        probability = model.predict_proba(processed_input)[:, 1]
        st.subheader("Recommendation:")
        st.write(f"Predicted Probability of Hiring: {probability[0]:.2f}")

        if probability[0] > 0.5: # Usando 0,5 como um limite simples
            st.success("Recommended: This candidate has a high probability of being hired for this job.")
        else:
            st.info("Not Recommended: This candidate has a lower probability of being hired for this job.")
    else:
        st.error(f"Input feature mismatch. Expected {model.n_features_in_} features, but got {processed_input.shape[1]}.")

"""
**Razão**:
O comando anterior falhou porque a biblioteca `streamlit` não está instalada. Instalei novamente  `streamlit` usando pip.

"""

!pip install streamlit

"""**Raciocínio**:
Agora que o `streamlit` está instalado, recrie o script Python para o aplicativo Streamlit.

"""

import streamlit as st
import joblib
import pandas as pd
import numpy as np

# Load the saved model and label encoders
try:
    model = joblib.load('random_forest_model.joblib')
    label_encoders = joblib.load('label_encoders.joblib')
except FileNotFoundError:
    st.error("Model or label encoders not found. Please run the training notebook first.")
    st.stop()

def preprocess_input(job_features, candidate_features, label_encoders):
    """
    Preprocesses job and candidate features using loaded label encoders.
    Ensures the input features are in the same format as the training data.
    """
    processed_features = {}
    # Combine job and candidate features into a single dictionary for easier processing
    all_features = {**job_features, **candidate_features}

    for col, le in label_encoders.items():
        value = all_features.get(col, '') # Get the value, default to empty string if not found
        try:
            # Use the loaded label encoder to transform the input value
            # Handle unseen values by assigning a default value (e.g., 0 or a dedicated 'unseen' category)
            # Here, we'll use 0, assuming the encoder was fitted on a comprehensive set
            processed_features[col + '_enc'] = le.transform([str(value).lower().strip()])[0]
        except ValueError:
            # Handle cases where transform fails (e.g., unseen label)
            processed_features[col + '_enc'] = 0 # Assign a default value

    # Convert to DataFrame, ensuring column order matches training data
    # We need to get the order of columns from the original X_final DataFrame used in training
    # Assuming the column order in X_final was based on the order in label_cols
    label_cols = list(label_encoders.keys())
    encoded_cols = [c + '_enc' for c in label_cols]
    processed_df = pd.DataFrame([processed_features], columns=encoded_cols)


    return processed_df

# Streamlit application layout
st.title("Candidate Recommendation Engine")

st.write("Enter job and candidate details to get a hiring recommendation.")

# Input widgets for job features (based on label_cols)
st.header("Job Details")
job_features = {}
for col in label_encoders.keys():
    if col.startswith('vaga_'):
        options = [''] + list(label_encoders[col].classes_) # Add empty string for selection
        job_features[col] = st.selectbox(f"Job {col.replace('vaga_', '').replace('_', ' ').title()}:", options)

# Input widgets for candidate features (based on label_cols)
st.header("Candidate Details")
candidate_features = {}
for col in label_encoders.keys():
    if col.startswith('candidato_'):
        options = [''] + list(label_encoders[col].classes_) # Add empty string for selection
        candidate_features[col] = st.selectbox(f"Candidate {col.replace('candidato_', '').replace('_', ' ').title()}:", options)


if st.button("Get Recommendation"):
    # Combine job and candidate features for preprocessing
    input_features = {**job_features, **candidate_features}

    # Preprocess the input data
    processed_input = preprocess_input(job_features, candidate_features, label_encoders)

    # Make prediction
    # Ensure the number of features matches the model's expectation
    # If the model was trained only on structured features, use processed_input directly
    # If the model was trained on structured + text features, we would need to process text here as well
    # Based on the previous notebook, the model was trained on structured features only.
    if processed_input.shape[1] == model.n_features_in_:
        probability = model.predict_proba(processed_input)[:, 1]
        st.subheader("Recommendation:")
        st.write(f"Predicted Probability of Hiring: {probability[0]:.2f}")

        if probability[0] > 0.5: # Using 0.5 as a simple threshold
            st.success("Recommended: This candidate has a high probability of being hired for this job.")
        else:
            st.info("Not Recommended: This candidate has a lower probability of being hired for this job.")
    else:
        st.error(f"Input feature mismatch. Expected {model.n_features_in_} features, but got {processed_input.shape[1]}.")

import streamlit as st
import joblib
import pandas as pd
import numpy as np

# Load the saved model and label encoders
try:
    model = joblib.load('random_forest_model.joblib')
    label_encoders = joblib.load('label_encoders.joblib')
except FileNotFoundError:
    st.error("Model or label encoders not found. Please run the training notebook first.")
    st.stop()

def preprocess_input(job_features, candidate_features, label_encoders):
    """
    Preprocesses job and candidate features using loaded label encoders.
    Ensures the input features are in the same format as the training data.
    """
    processed_features = {}
    # Combine job and candidate features into a single dictionary for easier processing
    all_features = {**job_features, **candidate_features}

    for col, le in label_encoders.items():
        value = all_features.get(col, '') # Get the value, default to empty string if not found
        try:
            # Use the loaded label encoder to transform the input value
            # Handle unseen values by assigning a default value (e.g., 0 or a dedicated 'unseen' category)
            # Here, we'll use 0, assuming the encoder was fitted on a comprehensive set
            processed_features[col + '_enc'] = le.transform([str(value).lower().strip()])[0]
        except ValueError:
            # Handle cases where transform fails (e.g., unseen label)
            processed_features[col + '_enc'] = 0 # Assign a default value

    # Convert to DataFrame, ensuring column order matches training data
    # We need to get the order of columns from the original X_final DataFrame used in training
    # Assuming the column order in X_final was based on the order in label_cols
    label_cols = list(label_encoders.keys())
    encoded_cols = [c + '_enc' for c in label_cols]
    processed_df = pd.DataFrame([processed_features], columns=encoded_cols)


    return processed_df

# Streamlit application layout
st.title("Candidate Recommendation Engine")

st.write("Enter job and candidate details to get a hiring recommendation.")

# Input widgets for job features (based on label_cols)
st.header("Job Details")
job_features = {}
for col in label_encoders.keys():
    if col.startswith('vaga_'):
        options = [''] + list(label_encoders[col].classes_) # Add empty string for selection
        job_features[col] = st.selectbox(f"Job {col.replace('vaga_', '').replace('_', ' ').title()}:", options)

# Input widgets for candidate features (based on label_cols)
st.header("Candidate Details")
candidate_features = {}
for col in label_encoders.keys():
    if col.startswith('candidato_'):
        options = [''] + list(label_encoders[col].classes_) # Add empty string for selection
        candidate_features[col] = st.selectbox(f"Candidate {col.replace('candidato_', '').replace('_', ' ').title()}:", options)


if st.button("Get Recommendation"):
    # Combine job and candidate features for preprocessing
    input_features = {**job_features, **candidate_features}

    # Preprocess the input data
    processed_input = preprocess_input(job_features, candidate_features, label_encoders)

    # Make prediction
    # Ensure the number of features matches the model's expectation
    # If the model was trained only on structured features, use processed_input directly
    # If the model was trained on structured + text features, we would need to process text here as well
    # Based on the previous notebook, the model was trained on structured features only.
    if processed_input.shape[1] == model.n_features_in_:
        probability = model.predict_proba(processed_input)[:, 1]
        st.subheader("Recommendation:")
        st.write(f"Predicted Probability of Hiring: {probability[0]:.2f}")

        if probability[0] > 0.5: # Using 0.5 as a simple threshold
            st.success("Recommended: This candidate has a high probability of being hired for this job.")
        else:
            st.info("Not Recommended: This candidate has a lower probability of being hired for this job.")
    else:
        st.error(f"Input feature mismatch. Expected {model.n_features_in_} features, but got {processed_input.shape[1]}.")

"""
## Configurei um ambiente de implantação

### Subtarefa:
Escolhi um método para implantar o aplicativo Streamlit. As opções comuns incluem Streamlit Cloud, Heroku ou um servidor personalizado."""